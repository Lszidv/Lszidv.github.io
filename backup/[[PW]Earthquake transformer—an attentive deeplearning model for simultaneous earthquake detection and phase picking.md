# 摘要
- 任务背景：在高噪声与微震监测条件下，地震检测与震相拾取难度大。
- 方法框架：提出可跨区域泛化的联合模型，同时执行“检测+拾取”，采用分层注意力融合“震相细节 + 全波形上下文”。
- 主要贡献：
  - 多任务学习提升各子任务表现（检测更稳、拾取更准）。
  - 在 2000 年鸟取地震连续 5 周数据上，仅用 <1/3 台站，事件检出与定位数提升约 2 倍。
  - P/S 到时精度接近人工，但效率与灵敏度更高，可识别更多、更小事件。
- 预期影响：改进地震目录完整性、支撑快速定位与时空演化分析，利于余震序列与触发机制研究。
---
# 研究动机与方法概述
- 任务现状：检测与拾取是 DL-地震学的核心热点；DL 相比传统方法具优势（稳健、自动化、泛化）。
- 难点差异：
  - 检测目标：降低 **$P(\text{FN})$** 与 **$P(\text{FP})$**，依赖全局多相上下文。
  - 拾取目标：提升到时精度 **$|\Delta t|$**，定位对 **$0.01\,\text{s}$** 级误差极敏感。
- 关键假设：上下文（全波形）可提升表征质量，且不同时间片段对任务的贡献不均衡。
- ## . 检测 vs 拾取：信息粒度与上下文
- **共同点**：都可视为**时序特征变化识别**问题。
- **差异点**：
  - 检测偏向**全局**：利用**整段波形**、多震相（含散射波）与**长时间上下文**；
  - 拾取偏向**局部**：聚焦**震相到时附近的狭窗**以实现高精度回归。
- **实践工作流（类人先验）**：  
  1) 先全局观察**多台站整段波形**，依据**震相时序先后**（如 P 先于 S、体波先于色散面波）判定“是否为地震”；  
  2) 再对**具体震相局部窗口**进行**精细到时**。  
  → 暗示“全局判别—局部精测”的**层级信息**是关键。
- 方法要点：引入双层注意力（Global + Local），显式建模“全局判别—局部精测”的人类工作流，提出 **EQTransformer** 实现单台近震的联合“检测+P/S拾取”。
- 评测设计：与多种 DL/传统方法对比，在日本 5 周连续数据上外部验证；通过“检测→定位”展示模型对区域泛化与震源刻画的改进。
---
# Results

## 结构总览
- 多任务框架：共享**超深编码器** + 三个**独立解码器**（检测 / P 相 / S 相）。
- 组件组合：一维卷积、双向/单向 **LSTM**、**Transformer/自注意**、**NIN**、**残差连接**、前馈层。
- 输出形式：逐时刻三条概率序列（地震存在、P 相、S 相）。

## 编码器（全局表征）
- 前端**下采样**（卷积 + MaxPool）以压缩序列长度 **$T$**，降低自注意内存与计算负担（典型 **$O(T^2)$**）。
- 主干由**残差 CNN + LSTM**堆叠，兼顾局部纹理与长时依赖。
- 末端**全局注意力**聚焦可能含有地震的时段，形成上下文加权表征。

## 解码器（任务专属）
- **检测分支**：高层特征 → 地震存在概率向量（全局触发曲线）。
- **P/S 分支**：入口置 **LSTM+局部注意**，在候选区间内对各震相微观特征精确聚焦；输出各自的逐时刻概率轨迹，服务到时拾取。

## 训练可优化性与效率
- **残差**与**NIN**支撑 56 层深度而**仅 ~ **$372\mathrm{K}$** 可训练参数**，实现高表示力与轻量化并存。
- 结构与超参由大量**原型网络试验**经验确定，兼顾三任务的精度、召回与到时误差。

## 设计动机与领域先验
- 模拟人类流程：**全局判别 → 局部精测**；以**全局注意**确定“在哪儿是地震”，再用**局部注意**精确到 **P/S**。
- 针对自注意的序列长度瓶颈，前置**下采样**平衡上下文范围与计算资源。
- P/S **分支解耦**，允许对两类相位的时频与包络差异进行专门优化。
<img width="841" height="1013" alt="Image" src="https://github.com/user-attachments/assets/4780bb80-2247-4879-ab6a-82b118513a47" />
---
# 数据与训练

## 1. 数据与预处理
- 数据源：**STEAD**（全球、标注齐全；不含日本记录）。  
- 规模与范围：地震 **1,000,000**，噪声 **300,000**，震中距 **≤ 300 km**，事件 **≈ 450,000**（多为 **$M<2.5$**；台站距震中 **≤ 100 km**）。  
- 划分：训练 **85%**／验证 **5%**／测试 **10%**（随机）。  
- 预处理：长度 **60 s**（**6000** 点，**$f_s=100\,\mathrm{Hz}$**），因果带通 **1–45 Hz**。

## 2. 标签策略
- **检测（binary）**：
  
$$ 
y(t) = 
\begin{cases} 
1, & t \in [t_P,\, t_S + 1.4\,(t_S - t_P)] \\ 
0, & \text{otherwise} 
\end{cases} 
$$

- **拾取（P/S 概率曲线）**：矩形/高斯/三角三选；最终采用**三角形**（更低 loss、更高 **F**）：  
  - 在 **$t_P$** 或 **$t_S$** 处峰值 **1**；到时**前后各 20 样本**线性衰减至 **0**（**$20/f_s=0.2\,\mathrm{s}$**）。

## 3. 训练配置
- 初始化：**Xavier 正态**；偏置 **0**。  
- 优化器：**Adam**，**学习率可变**日程。  
- 资源与时长：**4×V100**，**≈89 h**（TensorFlow）。  
- 早停：验证损失 **12** 个 epoch 无改进即停止。  
- Dropout：**0.1**（**训练/测试**皆启用，用于不确定性估计）。

## 4. 数据增强与并行管线
| 增强 | 目的 | 概率 |
|---|---|---|
| 次震叠加（空白段） | 重叠事件鲁棒性 | 0.3 |
| 高斯噪声注入 | 噪声鲁棒性 | 0.5 |
| 事件随机平移（数组旋转） | 时移不变性 | 0.99 |
| 噪声段插零间隙 | 缺失/跳变鲁棒性 | 0.2 |
| 丢弃 1–2 通道 | 传感器异常鲁棒性 | 0.3 |

- **半增强批**：每个 batch 中 **50%** 为增强版，维持分布平衡。  
- **并行预处理**：增强 + **按标准差归一化**在 **24 CPU** 上实时执行，GPU 训练无 I/O 阻塞。

## 5. 关键内联量
- **$t_P,t_S$**：P/S 到时  
- **$f_s=100\,\mathrm{Hz}$**：采样率  
- **$M<2.5$**：小震主导  
- **$1.4\,(t_S-t_P)$**：检测窗后延  
- **$0.2\,\mathrm{s}$**：三角标签两侧线性衰减时长
---
# 注意力机制可视化与解释

## 关键观点
- 分层注意力提供“**全局判别 → 局部精测**”的信息流：  
  - 全局注意：从整段波形中定位 **地震相关**片段；  
  - 局部注意：在事件片段内，聚焦 **P/S 到时**附近的窄窗特征。
- 检测分支更浅且标签覆盖时长更长 → **损失更大、梯度更强**，训练早期先学会“有无地震”的判别。
- 编码器末端的 **Transformer（I）** 充当信息路由，将“事件相关”上下文传递至解码器，抑制噪声干扰。

## 实务意义
- 注意力热力图能指示各任务的**关键时间片段**，辅助误检诊断与模型改进（如窗口策略、噪声建模）。  
- 层级注意符合人工工作流（先全局判断、后局部拾取），**提升可解释性与泛化性**。

## 术语内联
- **$c_t=\sum_j \alpha_{t,j}h_j$**（上下文向量）  
- **$\alpha_{t,j}$**（对齐分数，softmax 归一）  
- **$P(\mathrm{eq}\mid \mathbf{x})$**、**$P(P\mid \mathbf{x})$**、**$P(S\mid \mathbf{x})$**（条件概率式解释）
---
# 结果与方法对比

## 一、测试设置与可视化
- 统一评测：**113 K** 条 **1 min/3C** 波形（无额外滤波），对比**深度/传统**多法。
- 定性表现：**全局检测 + 高精度拾取**并存；可避免将强尾波/面波**过分裂**为多事件；对**高噪小震**鲁棒（图 4b–d）。
- **不确定性**有助于识别高概率但不可靠的样本（如 **MC dropout**）。

## 二、连续数据的工程流程
- 仅需：**补缝、去趋势、带通、重采样至 100 Hz**；切为 **1 min** 重叠窗；馈入时**归一化**。
- 训练增强对泛化**关键**：尽管训练数据多为“首 1/4 出现 P、每条仅 1 事件”，模型仍能处理**多事件**与**窗缘事件**（需 ≥ **0.2 s** 的 **P/S** 可见段），并抑制补缝突变导致的**误报**（文中图号处疑似笔误）。

## 三、检测性能
- 阈值 **0.5**：**FP=1，FN=0**（在 **113 K** 样本上）。  
- F1 对比：**EQTransformer** 全面领先；**CRED**（同样 CNN+RNN、同源 STEAD）仍不及；**注意力 + 更深网络**带来增益。  
- 现象：**STA/LTA** 在该测试上 **F1** 高于 **DetNet/Yews**，显示经典法在统一测试集上仍具竞争力；整体排序：**EQTransformer > STA/LTA >（DetNet, Yews）**。

## 四、拾取性能
- 对比对象：**PhaseNet、GPD、PpkNet、Yews、PickNet**（深度）与 **Kurtosis、Filter-Picker、AIC**（传统）。  
- 评价七项指标：**$sigma(\Delta t)$**、**$mu(\Delta t)$**、**Precision**、**Recall**、**F1**、**MAE**、**MAPE**。  
- **TP 定义**：**$|\hat t - t| < 0.5\,\mathrm{s}$**。  
- 结果：**P/S** 的 **F1** 均提升，**P 相**增益更显著；**S 相**较难、标注误差更大。  
- 误差分布：部分深度拾取器出现**非正态簇集**，可能与**滑窗策略**和**宽标签**有关（如 GPD/PpkNet/Yews），仍需消融确认。
---
# 区域外应用与结果

## 1. 测试设置
- 区域：日本西部鸟取 **Mw 6.6 (2000)** 余震区（**2000-10-06 ~ 2000-11-17**）。
- 台站：**18** 个 HiNet（JMA 研究原用 **57** 个）。
- 推断阈值：检测 **0.5**，P 拾取 **0.3**，S 拾取 **0.3**；窗重叠 **30%**，批量 **500**。
- 关联与定位：按检测时间进行事件关联；**HypoInverse** 定位，**HypoDD** 重定位（用**走时差 + 互相关**）。

## 2. 目录与完整性
- 事件数：**21,092**（> Fukuyama 等 **8,521** 的两倍），且仅用其**台站子集**。
- 覆盖：几乎包含 JMA 已报全部事件；约 **15%** 关联事件未入最终目录（与**关联策略简化**相关）。
- 完整性震级：  
  - **$M_c(\mathrm{JMA})\approx 1.82$**，**$M_c(\mathrm{EQTrans})\approx 1.50$**（最大曲率法）；  
  - 深度学习可探测**更小微震（\~20×）**，但降低总体 **$M_c$** 仍依赖**更密集台网**与**更好覆盖**。
- 频度-震级（G–R）框架（提示）：  

 $$
 log_{10}N(M)=a-bM,\quad M_c \text{ 为拐点/最大曲率处}
 $$

## 3. 拾取一致性
- 手动 vs 自动：在共有台站上对 **\~42k** 拾取对进行对齐评估。  
- 误差统计：  
  - **$sigma(\Delta t)\approx 0.08\,\mathrm{s}$**；**$mathrm{MAE}\approx 0.06\,\mathrm{s}$**（**6** 样本 @ **100 Hz**）；  
  - **均值误差 \~0.01 s（1 样本）**，系统偏差极小；  
  - **S 相**略优于 **P 相**（区域与标注因素所致）。

## 4. 实务要点
- **检测门控 + 相位阈值**策略：提高召回同时抑制误报。  
- **低台站数**条件下仍显著扩充目录，说明对**小震高噪**场景鲁棒。  
- **关联改进**方向：引入**多台站一致性/时空聚类/连通域**约束可降低“未入目录”的 15%。

## 5. 关键内联
- **$M_c$**（完整性震级）、**$b$** 值（G–R 斜率）、**$\Delta t$**（到时差）  
- **$f_s=100\,\mathrm{Hz}$**、**$1\,\mathrm{sample}=0.01\,\mathrm{s}$**

---
# 讨论

## 1. 性能增益来源
- 结构：更深网络 + **层级注意**（全局/局部）提升可分性与到时聚焦。
- 训练：**数据增强**与合理初始化/优化策略显著改善泛化。
- 数据：**高质量标注**与适当规模更关键，单纯扩量效益有限。

## 2. 不确定性与误报控制
- **认知不确定性（Epistemic）** 与误差不必然线性相关，但可用于**误报筛查**（高 **$P$** 伴随高方差→谨慎）。
- **随机不确定性（Aleatory）** 更契合置信区间，但在分类/到时联合任务中估计困难。
- **文化噪声**易致短窗误判；结合**长窗检测的高波动**可抑制 **FP**。
- 建议：扩充并**精标**人为/大气噪声；或在训练中显式引入**谱特征**。

## 3. 影响拾取稳定性的观测条件
- **远距/小震/低 SNR** → P 相概率降低、不确定性升高；**1 Hz 高通**不利于首达识别。
- 上述趋势对 **S 相**不如 P 相显著（与区域特性与标签噪声相关）。

## 4. 泛化与数据规模
- 区域限定训练亦可**跨区泛化**（如 PhaseNet）；多数情形**可直接部署**。
- 小规模但高质量/合适结构的模型（如 PpkNet）可优于大规模但不匹配的训练。

## 5. 标注与训练流程的重要性
- **软标签形状/窗策略/损失设计**决定峰值对齐与误差分布形态。
- 某些模型 P 相误差“簇集”可能源于**到时聚类偏置**（训练分布问题）。

## 6. 与传统方法对比
- 在**高噪**条件下，深度方法整体优于传统方法，**S 相**优势更明显。
- 实务建议：利用**不确定性阈** + **长/短窗互证** + **谱特征**，进一步降低 **$P(\mathrm{FP})$** 与 **$P(\mathrm{FN})$**。

## 1. 公平对比的挑战与倡议
- 模型差异体现在**标注、结构、超参、训练流程与数据质量**等多方面；统一评测困难。
- 倡导**统一基准**与**独立测试集**，推动可复现实验与严格对比。

## 2. 传统拾取器 vs 深度方法
- 传统法：到时**精度尚可**，但**召回低**、对 **S 相**较弱，误差**偏右（滞后）**。
- 速度对比（同一 i7/16 GB 机器）：Kurtosis **62 h12 m**、FilterPicker **3 h25 m**、AIC **31 h18 m**；**EQTransformer 2 h28 m**。
- 结论：深度模型在**吞吐与综合指标（F1）**上更具优势。

## 3. 日本应用的泛化与效率
- 到时精度≈人工，**检出事件数 >2×**；新增事件揭示**大山东坡**下稀疏地震性。
- 仅用**<1/3 台站**、较大台间距与**简单关联**即可达成；采用更强关联（如 **Glass3**）可进一步扩容。
- 工程效率：单核 CPU、单台站**1 个月**数据 **≈23 min**（不含不确定性估计）。

## 4. 方法学与贡献
- **层级注意 + 多任务**：全局判别与局部精测的有机结合（深编码器 + 三解码器）。
- **56 层**深网络、**贝叶斯不确定性**输出、**全球 1.2 M** 近震训练——支撑强泛化与高灵敏度。

## 5. 实务建议
- 对比评测务必**统一预处理与阈值策略**，并披露**训练数据质量与标注细节**。
- 在业务化部署中结合**高效关联算法**与**不确定性门控**，进一步压低 **误报/漏报** 并提升目录完整性。
---
# 方法：相关工作

## 1. 代表性工作脉络
- **CNN/FC 早期框架**：Perol³⁰（检测+聚类，区域内）、Ross¹⁰（P/S/噪声短窗检测）、Ross⁹（P 拾取）。
- **分割与端到端拾取**：Zhu & Beroza⁸（U-Net 端到端 P/S）。
- **时频与序列混合**：Mousavi⁷（ResCNN + BiLSTM，时频域，跨区/高噪泛化佳）。
- **两阶段拾取**：Pardo⁶（先分割后回归）。
- **区域数据实践**：Zhou⁵、Zhu⁴（汶川余震）；Dokht³（加拿大西部）；Wang²（日本，改 VGG-16，依赖理论到时短窗）。

## 2. 共性与差异
- **模型差异**：CNN/FC、U-Net、Res+RNN、BiGRU、VGG 变体、递归共享/专网分工。
- **数据差异**：规模（**$10^3\sim10^6$**）、震级/距震/类型/噪声/地域各异；预处理与增强策略不统一。
- **评测差异**：使用 **Accuracy/Precision/Recall/F1/AP/Hit/MAE/拾取误差** 等不一，难以横向对比。

## 3. 痛点与建议
- 痛点：缺少**高质量统一基准**导致**不可比**与**难以复用**。
- 建议：以 **STEAD** 作为**候选标准基准**，统一预处理与指标，促进**公平评测**与**社区共识**。

## 4. 评测指标提示
- **Precision/Recall/F1**（检测）  
  $$\mathrm{F1}=2\cdot\frac{\mathrm{Precision}\cdot\mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}$$
- **MAE**（拾取）  
  $$\mathrm{MAE}=\frac{1}{N}\sum_{i=1}^{N}\left|\hat t_i-t_i\right|$$

---
# 方法：网络设计

## 结构与动机
- 信号特性：地震波形兼具**局部震相细节**与**全局能量包络/多相耦合**。
- 设计目标：端到端模型中显式保留**局部—全局依赖的交互**，以提升检测与拾取的协同效果。

## 架构范式
- 采用 **CNN + RNN + Attention** 的**多任务**网络：  
  - 编码器：在**时域**抽取高层特征与**时序上下文**；  
  - 三个解码器：输出 **$p_{\mathrm{eq}}(t)$**（地震存在）、**$p_P(t)$**、**$p_S(t)$** 三条概率轨迹。
- 注意力机制：根据任务相关性**加权高亮/抑制**不同时间片，实现“全局判别—局部精测”的信息流。

## 计算考量
- 长时序 **RNN** 成本高，需在循环层前**下采样**以控制复杂度并稳定训练。
- **CNN×RNN** 组合已被实证为地震序列建模的有效方案，在此基础上引入注意力以进一步提升判别性。

## 关键内联
- **$p_{\mathrm{eq}}(t)$**、**$p_P(t)$**、**$p_S(t)$**：逐时刻概率序列  
- **$\alpha_t$**：注意力权重（时间片贡献）
---
# 方法：超深编码器

## 设计动机
- 更深网络 → 提升**表达力/泛化/抗噪**，对低 **SNR** 地震有效。
- 在**注意力**框架中，强表征的**深编码器**是性能关键。

## 结构要点
- 前端**下采样（Conv + MaxPool）**：缩短 **$T$**，降低注意力内存/算力。
- 主干：**残差卷积块** + **双向 LSTM 块**，并嵌入 **NIN** 增强非线性而**不显著增参**。
- 顺序：先 **CNN** 抽局部与时移不变性，再 **BiLSTM** 建模长依赖，后**自注意**做全局聚焦。

## 关键机理
- 卷积：利用局部结构与**时间平移不变性** → 泛化更佳。
- 残差：避免深度退化，稳定训练。
- **LSTM → Self-Attention**：前置 LSTM 注入**位置信息**，助力注意力对齐。

## 复杂度提示
$$
\text{Self-Attention Memory} \sim O(T^2)
$$

## 内联符号
- **$T$**：序列长度  
- **$h_t$**/**$c_t$**：LSTM 隐状态/记忆单元  
- **NIN**：逐点映射提高非线性，控制参数规模
---
# 注意力机制

## 1. 记号与核心公式
- 序列隐藏态：**$H=\{h_t\}\in\mathbb{R}^{n\times d_h}$**，上下文向量：**$c_t\in\mathbb{R}^{d_h}$**  
- 加性自注意（打分→归一→聚合）：

$$
e_{t,t'}=\sigma\!\Big(W_2^{\top}\big[\tanh(W_1^{\top}h_t+W_1^{\top}h_{t'}+b_1)\big]+b_2\Big)
$$

$$
\alpha_{t,t'}=\frac{\exp(e_{t,t'})}{\sum_{t'}\exp(e_{t,t'})},\qquad
c_t=\sum_{t'=1}^{n}\alpha_{t,t'}\,h_{t'}
$$

- 前馈层：

$$
FF(x)=\max(0,\,xW_1+b_1)\,W_2+b_2
$$

- 复杂度提示：**$O(n^2 d_h)$**（自注意）



## 2. 架构落地
- **残差注意力块**（Transformer 风格），但以**单头加性注意**替代多头点积注意；保留 **LayerNorm/残差 + FFN** 以增稳。
- **层级注意**（瓶颈处实施，控算量）：  
  - 全局注意（编码器末端）→ 识别**地震段**，路径短 + 检测损失强，先学会“有无地震”；  
  - 局部注意（各相位解码器起始）→ 在候选段内**锐化 P/S 聚焦**（只看小子集）。
- **位置信息**：每个层级的首个注意力块前加入 **1×LSTM(16)**，提供顺序/相对位置信号。

## 3. 直观理解（内联）
- **$h_t$**：局部状态；**$c_t$**：含全局上下文的加权状态；**$\alpha_{t,t'}$**：时间对齐“注意概率”。  
- **全局判别 → 局部精测**：先用全局注意找“哪里像地震”，再用局部注意在该段内锁定 **$P/S$** 到时。
---
# 不确定性估计

## 核心概念
- 预测**概率** ≠ 模型**置信**：高 **$ \text{softmax} $** 并不保证低不确定性。
- 目标：为地震**检测/拾取**提供**概率化输出 + 不确定性度量**，便于质控与误报抑制。

## 方法要点：MC Dropout（近似贝叶斯）
- 思想：在**推理期**也启用 **dropout**，相当于对权重施加伯努利随机掩码，进行**后验蒙特卡洛采样**。
- 多次随机前向得到一组预测 **$ \{ \hat{\mathbf{p}}^{(m)}(y\mid x) \}_{m=1}^{M} $**，据此估计均值与方差：
  
$$
  \bar{\mathbf{p}}(y\mid x)\approx \frac{1}{M}\sum_{m=1}^{M}\hat{\mathbf{p}}^{(m)}(y\mid x)
  $$
  
$$
  \mathrm{Var}[\mathbf{p}(y\mid x)]\approx \frac{1}{M}\sum_{m=1}^{M}\left(\hat{\mathbf{p}}^{(m)}(y\mid x)-\bar{\mathbf{p}}(y\mid x)\right)^{2}
  $$
  
- 实施细节：**全层**置入 dropout，**训练/预测**均启用；预测阶段通过重复采样获得**均值预测**与**不确定性**。

## 实务价值
- 利用**均值 + 方差**进行门控：**高概率但高方差** → 可疑样本，优先人工复核或提高阈值。
- 适用于 **P/S 拾取**与**事件检测**的**自动质控**与**假阳性抑制**。

## 关键内联
- **$ \bar{\mathbf{p}}(y\mid x) $**：MC 均值预测  
- **$ \mathrm{Var}[\mathbf{p}(y\mid x)] $**：MC 不确定性  
- **$ M $**：蒙特卡洛样本数（前向次数）



[Paper Link](https://www.nature.com/articles/s41467-020-17591-w)