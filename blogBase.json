{"singlePage": [], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "", "title": "\u829c\u5c3d", "subTitle": "Only love can set us free", "avatarUrl": "https://avatars.githubusercontent.com/u/160511559?s=400&u=68ec73daff523efd8652079b221d42e446d01cb6&v=4", "GMEEK_VERSION": "last", "postListJson": {"P1": {"htmlDir": "docs/post/From Here On.html", "labels": ["documentation"], "postTitle": "From Here On", "postUrl": "post/From%20Here%20On.html", "postSourceUrl": "https://github.com/Lszidv/Lszidv.github.io/issues/1", "commentNum": 0, "wordCount": 14, "description": "My first blog.\u3002", "top": 0, "createdAt": 1730102454, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/160511559?s=400&u=68ec73daff523efd8652079b221d42e446d01cb6&v=4", "createdDate": "2024-10-28", "dateLabelColor": "#bc4c00"}, "P2": {"htmlDir": "docs/post/[Literature Reading] Classification of Teleseismic Shear Wave Splitting Measurements- A Convolutional Neural Network Approach.html", "labels": ["documentation"], "postTitle": "[Literature Reading] Classification of Teleseismic Shear Wave Splitting Measurements: A Convolutional Neural Network Approach", "postUrl": "post/%5BLiterature%20Reading%5D%20Classification%20of%20Teleseismic%20Shear%20Wave%20Splitting%20Measurements-%20A%20Convolutional%20Neural%20Network%20Approach.html", "postSourceUrl": "https://github.com/Lszidv/Lszidv.github.io/issues/2", "commentNum": 1, "wordCount": 6151, "description": "# Classification of Teleseismic Shear Wave Splitting Measurements: A Convolutional Neural Network Approach\r\n# Abstract\r\n  \u526a\u5207\u6ce2\u5206\u88c2\r\n  \u95ee\u9898\uff1a\u9700\u8981\u53ef\u9760\u7684\u5206\u88c2\u6d4b\u91cf\u6570\u636e\uff0c\u76ee\u89c6\u6548\u7387\u4f4e\r\n  \u65b9\u6cd5\uff1aCNN \u4eba\u5de5\u8bc6\u522b\u6570\u636e\u8bad\u7ec3\uff0c\u5408\u6210\u6570\u636e\u6d4b\u8bd5\uff0c\u5b9e\u9645\u6570\u636e\u5bf9\u6bd4\r\n  \u5e94\u7528\uff1aBoardband seismic data recorded in south central Alaska\r\n\r\n# 1.Introduction\r\n  XKS\u6ce2\u5728\u5404\u5411\u5f02\u6027\u4ecb\u8d28\u4e2d\u4f1a\u5206\u88c2\u6210\u4e24\u4e2a\u6b63\u4ea4\u6781\u5316\u7684\u5feb\u6ce2\u548c\u6162\u6ce2\u3002", "top": 1, "createdAt": 1730108420, "style": "", "script": "<script>MathJax = {tex: {inlineMath: [[\"$\", \"$\"]]}};</script><script async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/160511559?s=400&u=68ec73daff523efd8652079b221d42e446d01cb6&v=4", "createdDate": "2024-10-28", "dateLabelColor": "#bc4c00"}, "P3": {"htmlDir": "docs/post/[Literaturre Reading] Making Reliable Shear-Wave Splitting Measurements.html", "labels": ["documentation"], "postTitle": "[Literaturre Reading] Making Reliable Shear-Wave Splitting Measurements", "postUrl": "post/%5BLiteraturre%20Reading%5D%20Making%20Reliable%20Shear-Wave%20Splitting%20Measurements.html", "postSourceUrl": "https://github.com/Lszidv/Lszidv.github.io/issues/3", "commentNum": 0, "description": "", "wordCount": 0, "top": 0, "createdAt": 1730711179, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/160511559?s=400&u=68ec73daff523efd8652079b221d42e446d01cb6&v=4", "createdDate": "2024-11-04", "dateLabelColor": "#bc4c00"}, "P4": {"htmlDir": "docs/post/CNN-SWS.html", "labels": ["Code"], "postTitle": "CNN-SWS", "postUrl": "post/CNN-SWS.html", "postSourceUrl": "https://github.com/Lszidv/Lszidv.github.io/issues/4", "commentNum": 0, "wordCount": 10762, "description": "## \u8bba\u6587\u201cClassification of Teleseismic Shear Wave Splitting Measurements: A Convolutional Neural Network Approach\u201d\u4ee3\u7801\u90e8\u5206\r\n\r\n### \u6587\u4ef6\u7ed3\u6784\r\n```\r\nCNN-SWS-main/\r\n\u251c\u2500\u2500 1_data/                                # \u6570\u636e\u6587\u4ef6\u5939\r\n\u2502   \u251c\u2500\u2500 Out_Bin/                           # \u5b58\u50a8 XKS.out \u6587\u4ef6\uff0c.out\u6587\u4ef6\u5305\u542b\u4e09\u5217\uff0c\u53f0\u7ad9\u548c\u7f51\u7edc\u540d\u79f0\uff08stname_NW\uff09\u3001\u4e8b\u4ef6\u540d\u79f0\uff08EQ123456789\uff09\u3001\u6d4b\u91cf\u8d28\u91cf\uff08A \u548c B \u8868\u793a\u53ef\u63a5\u53d7\uff0c\u5176\u4f59\u8868\u793a\u4e0d\u53ef\u63a5\u53d7\uff09      \r\n\u2502   \u2502   \u251c\u2500\u2500 PKS.out                       \r\n\u2502   \u2502   \u251c\u2500\u2500 SKK.out\r\n\u2502   \u2502   \u2514\u2500\u2500 SKS.out\r\n\u2502   \u2514\u2500\u2500 PKSOut/                            # \u5b58\u50a8\u4e0d\u540c\u53f0\u7ad9\u548c\u4e8b\u4ef6\u7684\u6ce2\u5f62\u6570\u636e\r\n\u2502       \u251c\u2500\u2500 109Cxx_TA/                     # \u53f0\u7ad9\u6587\u4ef6\u5939\r\n\u2502       \u2502   \u251c\u2500\u2500 EQ140250514/               # \u4e8b\u4ef6\u6587\u4ef6\u5939\r\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 109Cxx_TA.rl           # \u6821\u6b63\u5f84\u5411\u5206\u91cf\r\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 109Cxx_TA.ro           # \u539f\u59cb\u5f84\u5411\u5206\u91cf\r\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 109Cxx_TA.tl           # \u6821\u6b63\u6a2a\u5411\u5206\u91cf\r\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 109Cxx_TA.to           # \u539f\u59cb\u6a2a\u5411\u5206\u91cf\r\n\u2502       \u2502   \u251c\u2500\u2500 EQ********/               #\u5176\u4ed6\u4e8b\u4ef6\r\n\u2502       \u251c\u2500\u2500 **************                   # \u5176\u4ed6\u53f0\u7ad9\u6587\u4ef6\u5939\r\n\u2502   \u251c\u2500\u2500 PKS.list                               # Out_Bin/PKS.out\r\n\u2502   \u251c\u2500\u2500 SKK.list                               # Out_Bin/SKK.out\r\n\u2502   \u2514\u2500\u2500 SKS.list                               # Out_Bin/SKS.out\r\n\u2502\r\n\u251c\u2500\u2500 load/                                   # \u6570\u636e\u52a0\u8f7d\u548c\u9884\u6d4b\u6587\u4ef6\u5939\r\n\u2502   \u251c\u2500\u2500 2_load/                             # \u52a0\u8f7d\u8f93\u51fa\u6587\u4ef6\u5939\r\n\u2502   \u2502   \u2514\u2500\u2500 Outp/                           # \u5b58\u653e\u52a0\u8f7d\u9884\u6d4b\u7ed3\u679c\r\n\u2502   \u251c\u2500\u2500 load.py                             # \u6570\u636e\u52a0\u8f7d\u548c\u9884\u6d4b\u811a\u672c\r\n\u2502   \u2514\u2500\u2500 parameter.list                      # \u52a0\u8f7d\u8fc7\u7a0b\u53c2\u6570\r\n\u2502\r\n\u251c\u2500\u2500 model/                                  # \u6a21\u578b\u6587\u4ef6\u5939\r\n\u2502   \u2514\u2500\u2500 CNN_XKS.h5                          # \u5df2\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u6743\u91cd\r\n\u2502\r\n\u251c\u2500\u2500 train/                                  # \u6a21\u578b\u8bad\u7ec3\u6587\u4ef6\u5939\r\n\u2502   \u251c\u2500\u2500 2_train/                            # \u8bad\u7ec3\u8f93\u51fa\u6587\u4ef6\u5939\r\n\u2502   \u2502   \u251c\u2500\u2500 CNN_XKS.h5                      # \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u6743\u91cd\r\n\u2502   \u2502   \u251c\u2500\u2500 parameters.list                 # \u8bad\u7ec3\u8fc7\u7a0b\u53c2\u6570(\u5355\u72ec\u5199\u51fa\u6765\uff0c\u65b9\u4fbf\u6539\u52a8)\r\n\u2502   \u2502   \u251c\u2500\u2500 train_64.acc                    # \u8bad\u7ec3\u7cbe\u5ea6\u8bb0\u5f55\r\n\u2502   \u2502   \u251c\u2500\u2500 train_64.loss                   # \u8bad\u7ec3\u635f\u5931\u8bb0\u5f55\r\n\u2502   \u2502   \u251c\u2500\u2500 train_64.val_acc                # \u9a8c\u8bc1\u7cbe\u5ea6\u8bb0\u5f55\r\n\u2502   \u2502   \u2514\u2500\u2500 train_64.val_loss               # \u9a8c\u8bc1\u635f\u5931\u8bb0\u5f55\r\n\u2502   \u2514\u2500\u2500 train.py                            # \u6a21\u578b\u8bad\u7ec3\u811a\u672c\r\n\u2502\r\n\u251c\u2500\u2500 test/                                   # \u6d4b\u8bd5\u6587\u4ef6\u5939\r\n\u2502   \u2514\u2500\u2500 test.py                             # \u6a21\u578b\u53ef\u89c6\u5316\u548c\u6d4b\u8bd5\u811a\u672c\r\n\u2502\r\n\u251c\u2500\u2500 Do_load.cmd                             # \u52a0\u8f7d\u547d\u4ee4\u811a\u672c\r\n\u251c\u2500\u2500 Do_train.cmd                            # \u8bad\u7ec3\u547d\u4ee4\u811a\u672c\r\n\u2514\u2500\u2500 README.txt                              # \u9879\u76ee\u8bf4\u660e\u6587\u4ef6\r\n\r\n\r\n```\r\n\r\n### load.py\r\n  ```\r\nimport os\r\nimport numpy as np\r\nfrom obspy import read\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Conv1D, ZeroPadding1D, Flatten, Dense\r\n\r\nX = []                                                                        # \u6570\u636e\u5217\u8868\r\nY = []                                                                        # \u6807\u7b7e\u5217\u8868\r\nX_nst, Y_nev = [], []                                                       # \u53f0\u7ad9\u540d\u548c\u4e8b\u4ef6\u540d\r\ninput_length = 1000\r\n\r\n# \u6570\u636e\u548c\u6a21\u578b\u52a0\u8f7d\r\nnrt = os.path.normpath('C:/Users/~/S-wave spliting/CNN-SWS-main/1_data')\r\nnmodel = os.path.normpath('C:/Users/~/S-wave spliting/CNN-SWS-main/model/CNN_XKS.h5')\r\n# \u8def\u5f84\u68c0\u67e5\r\nif not os.path.exists(nrt):\r\n    raise FileNotFoundError(f'The data root path {nrt} does not exist.')\r\nif not os.path.exists(nmodel):\r\n    raise FileNotFoundError(f'The model path {nmodel} does not exist.')\r\n```\r\n\r\n```\r\n# \u8bfb\u53d6SAC\u6570\u636e\r\nXKS = ['PKS', 'SKS', 'SKK']\r\n\r\nfor k in range(3):\r\n    XKS_rout = os.path.join(nrt, f'{XKS[k]}.list')               # C:/Users/~/main/1_data/Out_Bin/*.out\r\n    print(f'Reading {XKS_rout}')\r\n\r\n    if not os.path.exists(XKS_rout):\r\n        raise FileNotFoundError(f'The file {XKS_rout} does not exist.')\r\n\r\n# \u9010\u884c\u8bfb\u53d6\u6570\u636e\r\nwith open(XKS_rout, 'r') as Pl:\r\n    for line_Pl in Pl:\r\n        vals = line_Pl.split()\r\n        P_rout = os.path.join(nrt, vals[0])                       # \u6570\u636e\u6839\u76ee\u5f55nrt \uff0b .out\u6587\u4ef6\u7b2c\u4e00\u5217\r\n        print(f'Doing: {XKS[k]} {vals[0]}')\r\n\r\n        if not os.path.exists(P_rout):\r\n            raise FileNotFoundError(f'The file {P_rout} does not exist.')\r\n```\r\n```\r\nPKS, y = [], []                                                              # PKS\u7528\u4e8e\u50a8\u5b58\u6ce2\u5f62\u6570\u636e\uff0cy\u50a8\u5b58\u5206\u7c7b\u6807\u7b7e\r\nwith open(P_rout, 'r') as P:\r\n    for line in P:\r\n        vals = line.split()\r\n        nst = vals[0]                                                       # station name\r\n        nev = vals[1]                                                      # event name\r\n\r\n        # \u5904\u7406\u5206\u7c7b\u6807\u7b7e\r\n        if vals[2] in ['A', 'B']:\r\n            y.append(1)\r\n            y.append(0)\r\n        else:\r\n            y.append(0)\r\n            y.append(1)\r\n\r\n```\r\n```\r\n\r\nncom = ['.ro', '.to', '.rl', '.tl']                                    # 4\u5206\u91cf\u5217\u8868    \r\ncomponents = []\r\nfor i in range(4):\r\n    ro_rout = os.path.join(nrt, f'{XKS[k]}Out', nst, nev, f'{nst}{ncom[i]}')\r\n    print(f'Reading file: {ro_rout}')\r\n\r\n    if os.path.exists(ro_rout):\r\n        st = read(ro_rout)\r\n        components.append(st[0].data[:input_length])   # \u622a\u53d6\u524dinput_length\u4e2a\u6570\u636e\r\n    else:\r\n        raise FileNotFoundError(f'The file {ro_rout} does not exist.')\r\n\r\nfor i in range(input_length):\r\n    PKS.append(np.array([comp[i] for comp in components]))\r\n\r\nX.append(np.array(PKS))                                            # \u5c064\u4e2a\u5206\u91cf\u7ec4\u6210\u7684PKS\u4fdd\u5b58\u5230\u5217\u8868X\u4e2d\r\nY.append(np.array(y))                                                # \u5206\u7c7b\u6807\u7b7e\u4fdd\u5b58\u5230Y\u4e2d\r\nX_nst.append(f'{nst}_{XKS[k]}_')                                # \u53f0\u7ad9\u4fe1\u606f\r\nY_nev.append(nev)                                                    # \u4e8b\u4ef6\u4fe1\u606f\r\n\r\n```\r\n\r\n```\r\n# \u5b9a\u4e49\u6a21\u578b\r\ninput_shape = (input_length, 4)\r\nmodel = Sequential()\r\n   # \u6dfb\u52a0\u5377\u79ef\u5c42\r\nmodel.add(Conv1D(kernel_size=3, filters=32, input_shape=input_shape, strides=2, activation='relu'))\r\nmodel.add(ZeroPadding1D(padding=1))\r\nmodel.add(Conv1D(kernel_size=3, filters=32, strides=2, activation='relu'))\r\nmodel.add(ZeroPadding1D(padding=1))\r\nmodel.add(Conv1D(kernel_size=3, filters=32, strides=2, activation='relu'))\r\nmodel.add(ZeroPadding1D(padding=1))\r\nmodel.add(Conv1D(kernel_size=3, filters=32, strides=2, activation='relu'))\r\nmodel.add(ZeroPadding1D(padding=1))\r\nmodel.add(Conv1D(kernel_size=3, filters=32, strides=2, activation='relu'))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(units=2, activation='softmax'))\r\n\r\n# \u52a0\u8f7d\u6a21\u578b\u6743\u91cd\u5e76\u8fdb\u884c\u9884\u6d4b\r\nmodel.load_weights(nmodel)\r\nresult = model.predict(np.array(X))\r\n\r\n# \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c\r\noutput_dir = os.path.join('C:/Users/~/S-wave spliting/CNN-SWS-main/load/2_load/Outp')\r\nos.makedirs(output_dir, exist_ok=True)\r\nfor i in range(len(result)):\r\n    nst = X_nst[i]                                                                        \r\n    nev = Y_nev[i]                                                                      \r\n    res_name = os.path.join(output_dir, f'{nst}_{nev}.res')         \r\n    y_name = os.path.join(output_dir, f'{nst}_{nev}.y')\r\n\r\n    np.savetxt(res_name, result[i])\r\n    np.savetxt(y_name, Y[i])\r\n\r\nprint('finish')\r\n```\r\n\r\n\r\n\r\n### train.py\r\n\r\n```\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport obspy\r\nimport csv\r\nfrom obspy import read\r\nfrom obspy.taup import TauPyModel\r\nimport os\r\nfrom pathlib import Path\r\nimport random\r\nimport keras\r\nfrom keras import regularizers\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, Conv1D, MaxPooling1D, UpSampling1D, ZeroPadding1D\r\n\r\n# \u521d\u59cb\u5316\r\nX_good, Y_good = [], []\r\nX_bad, Y_bad = [], []\r\nX, Y = [], []\r\nX_rand, Y_rand = [], []\r\nnst_good, nev_good = [], []\r\nnst_bad, nev_bad = [], []\r\nX_nst, Y_nev = [], []\r\nnst_rand, nev_rand = [], []\r\n```\r\n\r\n```\r\n# \u8bfb\u53d6\u53c2\u6570\u6587\u4ef6\r\nn=0\r\nwith open('parameters.list') as p:\r\n    for line in p:\r\n        n += 1\r\n        vals = line.split()\r\n        if n == 1: nrt = str(vals[0])\r\n        if n == 2: batch_size = int(vals[0])\r\n        if n == 3: epochs = int(vals[0])\r\n        if n == 4: byn = int(vals[0])\r\n        if n == 5:\r\n            ac = int(vals[0])\r\n            uc = int(vals[1])\r\n\r\n```\r\n\r\n```\r\n# \u8bfb\u53d6SAC\u6570\u636e\r\ninput_length = 1000\r\nXKS = ['PKS', 'SKS', 'SKK']\r\n\r\nfor k in range(3):\r\n    XKS_rout = nrt + str(XKS[k]) + '.list'\r\n    with open(XKS_rout) as Pl:\r\n        for line_Pl in Pl:\r\n            vals = line_Pl.split()\r\n            P_rout = nrt + str(vals[0])\r\n            with open(P_rout) as P:\r\n                for line in P:\r\n                    PKS, y = [], []\r\n                    vals = line.split()\r\n                    nst = vals[0]  # station name\r\n                    nev = vals[1]  # event name\r\n                    y = [1, 0] if vals[2] in ['A', 'B'] else [0, 1]\r\n                    \r\n                    ncom = ['.ro', '.to', '.rl', '.tl']\r\n                    for i in range(4):\r\n                        ro_rout = f'{nrt}{XKS[k]}Out/{nst}/{nev}/{nst}{ncom[i]}'\r\n                        st = read(ro_rout)\r\n                        if i == 0: ro = st[0].data\r\n                        if i == 1: to = st[0].data\r\n                        if i == 2: rl = st[0].data\r\n                        if i == 3: tl = st[0].data\r\n                        \r\n                    for i in range(input_length):\r\n                        PKS.append(np.array([ro[i], to[i], rl[i], tl[i]]))\r\n                    if y[0] == 1:\r\n                        X_good.append(PKS)\r\n                        Y_good.append(y)\r\n                        nst_good.append(f'{nst}_{XKS[k]}_')\r\n                        nev_good.append(nev)\r\n                    else:\r\n                        X_bad.append(PKS)\r\n                        Y_bad.append(y)\r\n                        nst_bad.append(f'{nst}_{XKS[k]}_')\r\n                        nev_bad.append(nev)\r\n```\r\n```\r\n# \u6570\u636e\u589e\u5f3a\uff08\u901a\u8fc7\u500d\u589e\u6765\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u7684\u7c7b\u522b\u6570\u91cf\uff09\r\nnpts = int(len(X_bad) / len(X_good))\r\nclass_weight = {0: ac, 1: uc}\r\nif byn == 0: npts = 1\r\n\r\nfor i in range(npts):\r\n    for ii in range(len(X_good)):\r\n        X.append(X_good[ii])\r\n        Y.append(Y_good[ii])\r\n        X_nst.append(nst_good[ii])\r\n        Y_nev.append(nev_good[ii])\r\n\r\nfor i in range(len(X_bad)):\r\n    X.append(X_bad[i])\r\n    Y.append(Y_bad[i])\r\n    X_nst.append(nst_bad[i])\r\n    Y_nev.append(nev_bad[i])\r\n\r\n# \u6570\u636e\u968f\u673a\u5316\u4e0e\u5212\u5206\r\nrann0 = random.sample(range(len(X)), len(X))\r\nX_rand = [X[i] for i in rann0]\r\nY_rand = [Y[i] for i in rann0]\r\nx_train, y_train = np.array(X_rand[:int(len(X) * 0.8)]), np.array(Y_rand[:int(len(Y) * 0.8)])\r\nx_test, y_test = np.array(X_rand[int(len(X) * 0.8):]), np.array(Y_rand[int(len(Y) * 0.8):])\r\n\r\n```\r\n```\r\nmodel = Sequential()\r\nmodel.add(Conv1D(32, kernel_size=3, strides=2, activation='relu', input_shape=(input_length, 4)))\r\nmodel.add(ZeroPadding1D(1))\r\n# \u6dfb\u52a0\u591a\u4e2a\u5377\u79ef\u5c42\uff0c\u6700\u7ec8\u5c55\u5e73\u6210\u5411\u91cf\u5e76\u8fde\u63a5\u5230 softmax \u8f93\u51fa\u5c42\r\nmodel.add(Flatten())\r\nmodel.add(Dense(2, activation='softmax'))\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\r\nH = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, class_weight=class_weight, validation_data=(x_test, y_test))\r\n\r\n# \u53ef\u89c6\u5316\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7cbe\u5ea6\r\nfig, ax = plt.subplots()\r\nplt.plot(H.history['acc'], label='train_acc')\r\nplt.plot(H.history['val_acc'], label='val_acc')\r\nplt.legend()\r\nplt.show()\r\n\r\nmodel.save_weights('CNN_XKS.h5')\r\nprint('Finish')\r\n\r\n```\r\n\r\n\u3002", "top": 0, "createdAt": 1730863802, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/160511559?s=400&u=68ec73daff523efd8652079b221d42e446d01cb6&v=4", "createdDate": "2024-11-06", "dateLabelColor": "#bc4c00"}, "P5": {"htmlDir": "docs/post/wei-shen-me-python-lei-zhong-yao-shi-yong-__init__()-te-shu-fang-fa.html", "labels": ["point"], "postTitle": "\u4e3a\u4ec0\u4e48python\u7c7b\u4e2d\u8981\u4f7f\u7528__init__()\u7279\u6b8a\u65b9\u6cd5", "postUrl": "post/wei-shen-me-python-lei-zhong-yao-shi-yong-__init__%28%29-te-shu-fang-fa.html", "postSourceUrl": "https://github.com/Lszidv/Lszidv.github.io/issues/5", "commentNum": 0, "wordCount": 1694, "description": "\u4eca\u5929\u770b\u5230\u5982\u4e0b\u4ee3\u7801\uff08\u4e00\u4e2a\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u7c7b\uff09\uff0c\u5f15\u53d1\u4e86\u7b14\u8005\u56f0\u6270\u5df2\u4e45\u7684\u95ee\u9898\uff1a__init__()\u7279\u6b8a\u65b9\u6cd5\u5230\u5e95\u6709\u4ec0\u4e48\u7528\uff0c\u4e3a\u4ec0\u4e48python\u7c7b\u4e2d\u8981\u4f7f\u7528__init__()\u7279\u6b8a\u65b9\u6cd5\uff1f\r\n```\r\nclass LRScheduler():\r\n\t'''\r\n\tLearning rate scheduler. If the validation loss does not decrease for the\r\n\tgiven number of `patience` epochs, then the learning rate will decrease by\r\n\tby given `factor`.\r\n\t'''\r\n\tdef __init__(self, optimizer, patience=7, min_lr=1e-6, factor=0.5):\r\n\t\t'''\r\n\t\tnew_lr = old_lr * factor\r\n\t\t:param optimizer: the optimizer we are using\r\n\t\t:param patience: how many epochs to wait before updating the lr\r\n\t\t:param min_lr: least lr value to reduce to while updating\r\n\t\t:param factor: factor by which the lr should be updated\r\n\t\t'''\r\n\t\tself.optimizer = optimizer\r\n\t\tself.patience = patience\r\n\t\tself.min_lr = min_lr\r\n\t\tself.factor = factor\r\n\t\tself.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n\t\t\t\tself.optimizer,\r\n\t\t\t\tmode='min',\r\n\t\t\t\tpatience=self.patience,\r\n\t\t\t\tfactor=self.factor,\r\n\t\t\t\tmin_lr=self.min_lr,\r\n\t\t\t\tverbose=True\r\n\t\t\t)\r\n\tdef __call__(self, val_loss):\r\n\t\tself.lr_scheduler.step(val_loss)\r\n```\r\n\r\n__init__\u662f\u4e00\u4e2a\u7279\u6b8a\u65b9\u6cd5\uff0c\u89e3\u91ca\u4e3a\u7c7b\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u6216\u6784\u9020\u5668\uff0c\u529f\u80fd\u4e5f\u5c31\u4e0d\u8a00\u800c\u55bb\u4e86\uff0c\u5f53\u521b\u5efa\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b\u65f6\uff0cpython\u4f1a\u81ea\u52a8\u8c03\u7528\u8fd9\u4e2a\u65b9\u6cd5\u3002", "top": 0, "createdAt": 1730981462, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/160511559?s=400&u=68ec73daff523efd8652079b221d42e446d01cb6&v=4", "createdDate": "2024-11-07", "dateLabelColor": "#bc4c00"}}, "singeListJson": {}, "labelColorDict": {"bug": "#d73a4a", "Code": "#0E8A16", "documentation": "#0075ca", "enhancement": "#a2eeef", "good first issue": "#7057ff", "help wanted": "#008672", "invalid": "#e4e669", "point": "#5319E7", "question": "#d876e3", "wontfix": "#ffffff"}, "displayTitle": "\u829c\u5c3d", "faviconUrl": "https://avatars.githubusercontent.com/u/160511559?s=400&u=68ec73daff523efd8652079b221d42e446d01cb6&v=4", "ogImage": "https://avatars.githubusercontent.com/u/160511559?s=400&u=68ec73daff523efd8652079b221d42e446d01cb6&v=4", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://Lszidv.github.io", "prevUrl": "disabled", "nextUrl": "disabled"}