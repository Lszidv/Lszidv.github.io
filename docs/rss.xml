<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>芜尽</title><link>https://Lszidv.github.io</link><description>Only love can set us free</description><copyright>芜尽</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/160511559?s=400&amp;u=68ec73daff523efd8652079b221d42e446d01cb6&amp;v=4</url><title>avatar</title><link>https://Lszidv.github.io</link></image><lastBuildDate>Fri, 15 Nov 2024 05:04:03 +0000</lastBuildDate><managingEditor>芜尽</managingEditor><ttl>60</ttl><webMaster>芜尽</webMaster><item><title>[Literature Reading]SWAS: A shear-wave analysis system for semi-automatic measurement of shear-wave splitting above small earthquakes</title><link>https://Lszidv.github.io/post/%5BLiterature%20Reading%5DSWAS-%20A%20shear-wave%20analysis%20system%20for%20semi-automatic%20measurement%20of%20shear-wave%20splitting%20above%20small%20earthquakes.html</link><description># SWAS: A shear-wave analysis system for semi-automatic measurement of shear-wave splitting above small earthquakes&#13;
&#13;
。</description><guid isPermaLink="true">https://Lszidv.github.io/post/%5BLiterature%20Reading%5DSWAS-%20A%20shear-wave%20analysis%20system%20for%20semi-automatic%20measurement%20of%20shear-wave%20splitting%20above%20small%20earthquakes.html</guid><pubDate>Fri, 15 Nov 2024 05:03:39 +0000</pubDate></item><item><title>[Literature Reading]Feasibility of Deep Learning in Shear Wave Splitting analysis using Synthetic-Data Training and Waveform Deconvolution</title><link>https://Lszidv.github.io/post/%5BLiterature%20Reading%5DFeasibility%20of%20Deep%20Learning%20in%20Shear%20Wave%20Splitting%20analysis%20using%20Synthetic-Data%20Training%20and%20Waveform%20Deconvolution.html</link><description>## Feasibility of Deep Learning in Shear Wave Splitting analysis using Synthetic-Data Training and Waveform Deconvolution&#13;
。</description><guid isPermaLink="true">https://Lszidv.github.io/post/%5BLiterature%20Reading%5DFeasibility%20of%20Deep%20Learning%20in%20Shear%20Wave%20Splitting%20analysis%20using%20Synthetic-Data%20Training%20and%20Waveform%20Deconvolution.html</guid><pubDate>Thu, 14 Nov 2024 02:54:56 +0000</pubDate></item><item><title>为什么python类中要使用__init__()特殊方法</title><link>https://Lszidv.github.io/post/wei-shen-me-python-lei-zhong-yao-shi-yong-__init__%28%29-te-shu-fang-fa.html</link><description>今天看到如下代码（一个学习率调度器类），引发了笔者困扰已久的问题：__init__()特殊方法到底有什么用，为什么python类中要使用__init__()特殊方法？&#13;
```&#13;
class LRScheduler():&#13;
	'''&#13;
	Learning rate scheduler. If the validation loss does not decrease for the&#13;
	given number of `patience` epochs, then the learning rate will decrease by&#13;
	by given `factor`.&#13;
	'''&#13;
	def __init__(self, optimizer, patience=7, min_lr=1e-6, factor=0.5):&#13;
		'''&#13;
		new_lr = old_lr * factor&#13;
		:param optimizer: the optimizer we are using&#13;
		:param patience: how many epochs to wait before updating the lr&#13;
		:param min_lr: least lr value to reduce to while updating&#13;
		:param factor: factor by which the lr should be updated&#13;
		'''&#13;
		self.optimizer = optimizer&#13;
		self.patience = patience&#13;
		self.min_lr = min_lr&#13;
		self.factor = factor&#13;
		self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(&#13;
				self.optimizer,&#13;
				mode='min',&#13;
				patience=self.patience,&#13;
				factor=self.factor,&#13;
				min_lr=self.min_lr,&#13;
				verbose=True&#13;
			)&#13;
	def __call__(self, val_loss):&#13;
		self.lr_scheduler.step(val_loss)&#13;
```&#13;
&#13;
__init__是一个特殊方法，解释为类的初始化方法或构造器，功能也就不言而喻了，当创建一个类的实例时，python会自动调用这个方法。</description><guid isPermaLink="true">https://Lszidv.github.io/post/wei-shen-me-python-lei-zhong-yao-shi-yong-__init__%28%29-te-shu-fang-fa.html</guid><pubDate>Thu, 07 Nov 2024 12:11:02 +0000</pubDate></item><item><title>CNN-SWS</title><link>https://Lszidv.github.io/post/CNN-SWS.html</link><description>## 论文“Classification of Teleseismic Shear Wave Splitting Measurements: A Convolutional Neural Network Approach”代码部分&#13;
&#13;
### 文件结构&#13;
```&#13;
CNN-SWS-main/&#13;
├── 1_data/                                # 数据文件夹&#13;
│   ├── Out_Bin/                           # 存储 XKS.out 文件，.out文件包含三列，台站和网络名称（stname_NW）、事件名称（EQ123456789）、测量质量（A 和 B 表示可接受，其余表示不可接受）      &#13;
│   │   ├── PKS.out                       &#13;
│   │   ├── SKK.out&#13;
│   │   └── SKS.out&#13;
│   └── PKSOut/                            # 存储不同台站和事件的波形数据&#13;
│       ├── 109Cxx_TA/                     # 台站文件夹&#13;
│       │   ├── EQ140250514/               # 事件文件夹&#13;
│       │   │   ├── 109Cxx_TA.rl           # 校正径向分量&#13;
│       │   │   ├── 109Cxx_TA.ro           # 原始径向分量&#13;
│       │   │   ├── 109Cxx_TA.tl           # 校正横向分量&#13;
│       │   │   └── 109Cxx_TA.to           # 原始横向分量&#13;
│       │   ├── EQ********/               #其他事件&#13;
│       ├── **************                   # 其他台站文件夹&#13;
│   ├── PKS.list                               # Out_Bin/PKS.out&#13;
│   ├── SKK.list                               # Out_Bin/SKK.out&#13;
│   └── SKS.list                               # Out_Bin/SKS.out&#13;
│&#13;
├── load/                                   # 数据加载和预测文件夹&#13;
│   ├── 2_load/                             # 加载输出文件夹&#13;
│   │   └── Outp/                           # 存放加载预测结果&#13;
│   ├── load.py                             # 数据加载和预测脚本&#13;
│   └── parameter.list                      # 加载过程参数&#13;
│&#13;
├── model/                                  # 模型文件夹&#13;
│   └── CNN_XKS.h5                          # 已训练好的模型权重&#13;
│&#13;
├── train/                                  # 模型训练文件夹&#13;
│   ├── 2_train/                            # 训练输出文件夹&#13;
│   │   ├── CNN_XKS.h5                      # 训练后的模型权重&#13;
│   │   ├── parameters.list                 # 训练过程参数(单独写出来，方便改动)&#13;
│   │   ├── train_64.acc                    # 训练精度记录&#13;
│   │   ├── train_64.loss                   # 训练损失记录&#13;
│   │   ├── train_64.val_acc                # 验证精度记录&#13;
│   │   └── train_64.val_loss               # 验证损失记录&#13;
│   └── train.py                            # 模型训练脚本&#13;
│&#13;
├── test/                                   # 测试文件夹&#13;
│   └── test.py                             # 模型可视化和测试脚本&#13;
│&#13;
├── Do_load.cmd                             # 加载命令脚本&#13;
├── Do_train.cmd                            # 训练命令脚本&#13;
└── README.txt                              # 项目说明文件&#13;
&#13;
&#13;
```&#13;
&#13;
### load.py&#13;
  ```&#13;
import os&#13;
import numpy as np&#13;
from obspy import read&#13;
from keras.models import Sequential&#13;
from keras.layers import Conv1D, ZeroPadding1D, Flatten, Dense&#13;
&#13;
X = []                                                                        # 数据列表&#13;
Y = []                                                                        # 标签列表&#13;
X_nst, Y_nev = [], []                                                       # 台站名和事件名&#13;
input_length = 1000&#13;
&#13;
# 数据和模型加载&#13;
nrt = os.path.normpath('C:/Users/~/S-wave spliting/CNN-SWS-main/1_data')&#13;
nmodel = os.path.normpath('C:/Users/~/S-wave spliting/CNN-SWS-main/model/CNN_XKS.h5')&#13;
# 路径检查&#13;
if not os.path.exists(nrt):&#13;
    raise FileNotFoundError(f'The data root path {nrt} does not exist.')&#13;
if not os.path.exists(nmodel):&#13;
    raise FileNotFoundError(f'The model path {nmodel} does not exist.')&#13;
```&#13;
&#13;
```&#13;
# 读取SAC数据&#13;
XKS = ['PKS', 'SKS', 'SKK']&#13;
&#13;
for k in range(3):&#13;
    XKS_rout = os.path.join(nrt, f'{XKS[k]}.list')               # C:/Users/~/main/1_data/Out_Bin/*.out&#13;
    print(f'Reading {XKS_rout}')&#13;
&#13;
    if not os.path.exists(XKS_rout):&#13;
        raise FileNotFoundError(f'The file {XKS_rout} does not exist.')&#13;
&#13;
# 逐行读取数据&#13;
with open(XKS_rout, 'r') as Pl:&#13;
    for line_Pl in Pl:&#13;
        vals = line_Pl.split()&#13;
        P_rout = os.path.join(nrt, vals[0])                       # 数据根目录nrt ＋ .out文件第一列&#13;
        print(f'Doing: {XKS[k]} {vals[0]}')&#13;
&#13;
        if not os.path.exists(P_rout):&#13;
            raise FileNotFoundError(f'The file {P_rout} does not exist.')&#13;
```&#13;
```&#13;
PKS, y = [], []                                                              # PKS用于储存波形数据，y储存分类标签&#13;
with open(P_rout, 'r') as P:&#13;
    for line in P:&#13;
        vals = line.split()&#13;
        nst = vals[0]                                                       # station name&#13;
        nev = vals[1]                                                      # event name&#13;
&#13;
        # 处理分类标签&#13;
        if vals[2] in ['A', 'B']:&#13;
            y.append(1)&#13;
            y.append(0)&#13;
        else:&#13;
            y.append(0)&#13;
            y.append(1)&#13;
&#13;
```&#13;
```&#13;
&#13;
ncom = ['.ro', '.to', '.rl', '.tl']                                    # 4分量列表    &#13;
components = []&#13;
for i in range(4):&#13;
    ro_rout = os.path.join(nrt, f'{XKS[k]}Out', nst, nev, f'{nst}{ncom[i]}')&#13;
    print(f'Reading file: {ro_rout}')&#13;
&#13;
    if os.path.exists(ro_rout):&#13;
        st = read(ro_rout)&#13;
        components.append(st[0].data[:input_length])   # 截取前input_length个数据&#13;
    else:&#13;
        raise FileNotFoundError(f'The file {ro_rout} does not exist.')&#13;
&#13;
for i in range(input_length):&#13;
    PKS.append(np.array([comp[i] for comp in components]))&#13;
&#13;
X.append(np.array(PKS))                                            # 将4个分量组成的PKS保存到列表X中&#13;
Y.append(np.array(y))                                                # 分类标签保存到Y中&#13;
X_nst.append(f'{nst}_{XKS[k]}_')                                # 台站信息&#13;
Y_nev.append(nev)                                                    # 事件信息&#13;
&#13;
```&#13;
&#13;
```&#13;
# 定义模型&#13;
input_shape = (input_length, 4)&#13;
model = Sequential()&#13;
   # 添加卷积层&#13;
model.add(Conv1D(kernel_size=3, filters=32, input_shape=input_shape, strides=2, activation='relu'))&#13;
model.add(ZeroPadding1D(padding=1))&#13;
model.add(Conv1D(kernel_size=3, filters=32, strides=2, activation='relu'))&#13;
model.add(ZeroPadding1D(padding=1))&#13;
model.add(Conv1D(kernel_size=3, filters=32, strides=2, activation='relu'))&#13;
model.add(ZeroPadding1D(padding=1))&#13;
model.add(Conv1D(kernel_size=3, filters=32, strides=2, activation='relu'))&#13;
model.add(ZeroPadding1D(padding=1))&#13;
model.add(Conv1D(kernel_size=3, filters=32, strides=2, activation='relu'))&#13;
model.add(Flatten())&#13;
model.add(Dense(units=2, activation='softmax'))&#13;
&#13;
# 加载模型权重并进行预测&#13;
model.load_weights(nmodel)&#13;
result = model.predict(np.array(X))&#13;
&#13;
# 保存预测结果&#13;
output_dir = os.path.join('C:/Users/~/S-wave spliting/CNN-SWS-main/load/2_load/Outp')&#13;
os.makedirs(output_dir, exist_ok=True)&#13;
for i in range(len(result)):&#13;
    nst = X_nst[i]                                                                        &#13;
    nev = Y_nev[i]                                                                      &#13;
    res_name = os.path.join(output_dir, f'{nst}_{nev}.res')         &#13;
    y_name = os.path.join(output_dir, f'{nst}_{nev}.y')&#13;
&#13;
    np.savetxt(res_name, result[i])&#13;
    np.savetxt(y_name, Y[i])&#13;
&#13;
print('finish')&#13;
```&#13;
&#13;
&#13;
&#13;
### train.py&#13;
&#13;
```&#13;
import numpy as np&#13;
import matplotlib.pyplot as plt&#13;
import obspy&#13;
import csv&#13;
from obspy import read&#13;
from obspy.taup import TauPyModel&#13;
import os&#13;
from pathlib import Path&#13;
import random&#13;
import keras&#13;
from keras import regularizers&#13;
from keras.models import Sequential&#13;
from keras.layers import Dense, Dropout, Flatten, Conv2D, Conv1D, MaxPooling1D, UpSampling1D, ZeroPadding1D&#13;
&#13;
# 初始化&#13;
X_good, Y_good = [], []&#13;
X_bad, Y_bad = [], []&#13;
X, Y = [], []&#13;
X_rand, Y_rand = [], []&#13;
nst_good, nev_good = [], []&#13;
nst_bad, nev_bad = [], []&#13;
X_nst, Y_nev = [], []&#13;
nst_rand, nev_rand = [], []&#13;
```&#13;
&#13;
```&#13;
# 读取参数文件&#13;
n=0&#13;
with open('parameters.list') as p:&#13;
    for line in p:&#13;
        n += 1&#13;
        vals = line.split()&#13;
        if n == 1: nrt = str(vals[0])&#13;
        if n == 2: batch_size = int(vals[0])&#13;
        if n == 3: epochs = int(vals[0])&#13;
        if n == 4: byn = int(vals[0])&#13;
        if n == 5:&#13;
            ac = int(vals[0])&#13;
            uc = int(vals[1])&#13;
&#13;
```&#13;
&#13;
```&#13;
# 读取SAC数据&#13;
input_length = 1000&#13;
XKS = ['PKS', 'SKS', 'SKK']&#13;
&#13;
for k in range(3):&#13;
    XKS_rout = nrt + str(XKS[k]) + '.list'&#13;
    with open(XKS_rout) as Pl:&#13;
        for line_Pl in Pl:&#13;
            vals = line_Pl.split()&#13;
            P_rout = nrt + str(vals[0])&#13;
            with open(P_rout) as P:&#13;
                for line in P:&#13;
                    PKS, y = [], []&#13;
                    vals = line.split()&#13;
                    nst = vals[0]  # station name&#13;
                    nev = vals[1]  # event name&#13;
                    y = [1, 0] if vals[2] in ['A', 'B'] else [0, 1]&#13;
                    &#13;
                    ncom = ['.ro', '.to', '.rl', '.tl']&#13;
                    for i in range(4):&#13;
                        ro_rout = f'{nrt}{XKS[k]}Out/{nst}/{nev}/{nst}{ncom[i]}'&#13;
                        st = read(ro_rout)&#13;
                        if i == 0: ro = st[0].data&#13;
                        if i == 1: to = st[0].data&#13;
                        if i == 2: rl = st[0].data&#13;
                        if i == 3: tl = st[0].data&#13;
                        &#13;
                    for i in range(input_length):&#13;
                        PKS.append(np.array([ro[i], to[i], rl[i], tl[i]]))&#13;
                    if y[0] == 1:&#13;
                        X_good.append(PKS)&#13;
                        Y_good.append(y)&#13;
                        nst_good.append(f'{nst}_{XKS[k]}_')&#13;
                        nev_good.append(nev)&#13;
                    else:&#13;
                        X_bad.append(PKS)&#13;
                        Y_bad.append(y)&#13;
                        nst_bad.append(f'{nst}_{XKS[k]}_')&#13;
                        nev_bad.append(nev)&#13;
```&#13;
```&#13;
# 数据增强（通过倍增来平衡数据集中的类别数量）&#13;
npts = int(len(X_bad) / len(X_good))&#13;
class_weight = {0: ac, 1: uc}&#13;
if byn == 0: npts = 1&#13;
&#13;
for i in range(npts):&#13;
    for ii in range(len(X_good)):&#13;
        X.append(X_good[ii])&#13;
        Y.append(Y_good[ii])&#13;
        X_nst.append(nst_good[ii])&#13;
        Y_nev.append(nev_good[ii])&#13;
&#13;
for i in range(len(X_bad)):&#13;
    X.append(X_bad[i])&#13;
    Y.append(Y_bad[i])&#13;
    X_nst.append(nst_bad[i])&#13;
    Y_nev.append(nev_bad[i])&#13;
&#13;
# 数据随机化与划分&#13;
rann0 = random.sample(range(len(X)), len(X))&#13;
X_rand = [X[i] for i in rann0]&#13;
Y_rand = [Y[i] for i in rann0]&#13;
x_train, y_train = np.array(X_rand[:int(len(X) * 0.8)]), np.array(Y_rand[:int(len(Y) * 0.8)])&#13;
x_test, y_test = np.array(X_rand[int(len(X) * 0.8):]), np.array(Y_rand[int(len(Y) * 0.8):])&#13;
&#13;
```&#13;
```&#13;
model = Sequential()&#13;
model.add(Conv1D(32, kernel_size=3, strides=2, activation='relu', input_shape=(input_length, 4)))&#13;
model.add(ZeroPadding1D(1))&#13;
# 添加多个卷积层，最终展平成向量并连接到 softmax 输出层&#13;
model.add(Flatten())&#13;
model.add(Dense(2, activation='softmax'))&#13;
&#13;
model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])&#13;
H = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, class_weight=class_weight, validation_data=(x_test, y_test))&#13;
&#13;
# 可视化训练和验证精度&#13;
fig, ax = plt.subplots()&#13;
plt.plot(H.history['acc'], label='train_acc')&#13;
plt.plot(H.history['val_acc'], label='val_acc')&#13;
plt.legend()&#13;
plt.show()&#13;
&#13;
model.save_weights('CNN_XKS.h5')&#13;
print('Finish')&#13;
&#13;
```&#13;
&#13;
。</description><guid isPermaLink="true">https://Lszidv.github.io/post/CNN-SWS.html</guid><pubDate>Wed, 06 Nov 2024 03:30:02 +0000</pubDate></item><item><title>[Literaturre Reading] Making Reliable Shear-Wave Splitting Measurements</title><link>https://Lszidv.github.io/post/%5BLiteraturre%20Reading%5D%20Making%20Reliable%20Shear-Wave%20Splitting%20Measurements.html</link><guid isPermaLink="true">https://Lszidv.github.io/post/%5BLiteraturre%20Reading%5D%20Making%20Reliable%20Shear-Wave%20Splitting%20Measurements.html</guid><pubDate>Mon, 04 Nov 2024 09:06:19 +0000</pubDate></item><item><title>[Literature Reading] Classification of Teleseismic Shear Wave Splitting Measurements: A Convolutional Neural Network Approach</title><link>https://Lszidv.github.io/post/%5BLiterature%20Reading%5D%20Classification%20of%20Teleseismic%20Shear%20Wave%20Splitting%20Measurements-%20A%20Convolutional%20Neural%20Network%20Approach.html</link><description># Classification of Teleseismic Shear Wave Splitting Measurements: A Convolutional Neural Network Approach&#13;
# Abstract&#13;
  剪切波分裂&#13;
  问题：需要可靠的分裂测量数据，目视效率低&#13;
  方法：CNN 人工识别数据训练，合成数据测试，实际数据对比&#13;
  应用：Boardband seismic data recorded in south central Alaska&#13;
&#13;
# 1.Introduction&#13;
  XKS波在各向异性介质中会分裂成两个正交极化的快波和慢波。</description><guid isPermaLink="true">https://Lszidv.github.io/post/%5BLiterature%20Reading%5D%20Classification%20of%20Teleseismic%20Shear%20Wave%20Splitting%20Measurements-%20A%20Convolutional%20Neural%20Network%20Approach.html</guid><pubDate>Mon, 28 Oct 2024 09:40:20 +0000</pubDate></item><item><title>From Here On</title><link>https://Lszidv.github.io/post/From%20Here%20On.html</link><description>My first blog.。</description><guid isPermaLink="true">https://Lszidv.github.io/post/From%20Here%20On.html</guid><pubDate>Mon, 28 Oct 2024 08:00:54 +0000</pubDate></item></channel></rss>